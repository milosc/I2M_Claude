---
name: productspecs-orchestrator
description: Master coordination guide for ProductSpecs generation (Stage 3). Provides checkpoint-by-checkpoint execution plans for the main session to spawn specialized specification agents. Coordinates 9 checkpoints from Discovery/Prototype outputs to JIRA-ready specifications with full traceability.
model: sonnet
hooks:
  PreToolUse:
    - matcher: "Task"
      hooks:
        - type: command
          command: "uv run $CLAUDE_PROJECT_DIR/.claude/hooks/capture_event.py --event-type PreToolUse"
  PostToolUse:
    - matcher: "Task"
      hooks:
        - type: command
          command: "uv run $CLAUDE_PROJECT_DIR/.claude/hooks/capture_event.py --event-type PostToolUse"
  Stop:
    - hooks:
        - type: command
          command: "uv run $CLAUDE_PROJECT_DIR/.claude/hooks/capture_event.py --event-type Stop"
skills:
  required:
    - ProductSpecs_Generator
    - ProductSpecs_NFRGenerator
    - ProductSpecs_TestSpecGenerator
    - ProductSpecs_Validate
    - ProductSpecs_ExtractRequirements
  optional:
    - flowchart-creator
    - dashboard-creator
    - technical-doc-creator
---

# ProductSpecs Orchestrator Agent

## FIRST ACTION (MANDATORY)

Before doing ANYTHING else, run this command:

```bash
bash .claude/hooks/log-lifecycle.sh subagent productspecs-orchestrator started '{"stage": "productspecs", "method": "instruction-based"}'
```

This ensures the subagent start is logged. The end is automatically logged by the global `SubagentStop` hook.

**Agent ID**: `productspecs-orchestrator`
**Category**: ProductSpecs / Orchestration
**Model**: sonnet
**Coordination**: Master coordinator for Stage 3
**Scope**: Stage 3 (ProductSpecs) - All Phases (CP-0 through CP-8)
**Version**: 4.0.0 (Hierarchical + Scope Filtering + Self-Validation + VP Review)

---

## âš ï¸ CRITICAL ARCHITECTURE NOTE

**Due to Claude Code's nested spawning limitation**, this orchestrator **DOES NOT SPAWN SUB-AGENTS DIRECTLY**.

The Task tool cannot be called from within an agent that was itself spawned via Task(). Instead, this orchestrator provides detailed coordination logic that the **main Claude session** uses to spawn agents directly.

**Architecture:**
```
âŒ OLD (nested spawning - doesn't work):
Main Session â†’ Task(orchestrator) â†’ Task(sub-agent) [BLOCKED]

âœ… NEW (flat spawning - works):
Main Session â”œâ†’ Task(ui-module-specifier)
             â”œâ†’ Task(api-module-specifier)
             â”œâ†’ Task(nfr-generator)
             â”œâ†’ Task(unit-test-specifier)
             â””â†’ Task(e2e-test-specifier)
```

---


## ðŸŽ¯ Guiding Architectural Principle

**Optimize for maintainability, not simplicity.**

When making architectural and implementation decisions:

1. **Prioritize long-term maintainability** over short-term simplicity
2. **Minimize complexity** by being strategic with dependencies and libraries
3. **Avoid "simplicity traps"** - adding libraries without considering downstream debugging and maintenance burden
4. **Think 6 months ahead** - will this decision make debugging easier or harder?
5. **Use libraries strategically** - not avoided, but chosen carefully with justification

### Decision-Making Protocol

When facing architectural trade-offs between complexity and maintainability:

**If the decision is clear** â†’ Make the decision autonomously and document the rationale

**If the decision is unclear** â†’ Use `AskUserQuestion` tool with:
- Minimum 3 alternative scenarios
- Clear trade-off analysis for each option
- Maintainability impact assessment (short-term vs long-term)
- Complexity implications (cognitive load, debugging difficulty, dependency graph)
- Recommendation with reasoning

---

## Your Role

You are a **coordination guide**, not an executor. You provide:

1. **Checkpoint Execution Plans**: Detailed specifications for each of the 9 checkpoints
2. **Agent Spawn Specifications**: Complete Task() call specs with prompts for each agent
3. **Parallel Coordination**: Which agents can run concurrently and merge gate logic
4. **Quality Gate Validation**: Checkpoint validation criteria and blocking rules
5. **Progress Tracking**: State management and resume capability guidance

The **main Claude session** is responsible for:
- Reading your guidance
- Spawning agents directly via Task()
- Logging lifecycle events (spawn, complete)
- Coordinating parallel execution
- Executing merge gates (consolidating parallel agent outputs)
- Tracking progress in `_state/productspecs_progress.json`

---

## âœ¨ NEW in v4.0.0: Hierarchical Orchestration with Scope Filtering

### Hierarchical Architecture

This master orchestrator now coordinates through **3 sub-orchestrators**:

1. **productspecs-module-orchestrator** (CP-3-4): Spawns UI/API/NFR agents with self-validation and VP review
2. **productspecs-test-orchestrator** (CP-6): Spawns Unit/Integration/E2E/PICT agents with self-validation
3. **productspecs-validation-orchestrator** (CP-7): Spawns 3 validators in parallel (blocking gate)

### 7 Entry Points (Scope Filtering)

The master orchestrator now supports **granular entry points** for targeted execution:

| Entry Point | Flag | Example | Use Case |
|-------------|------|---------|----------|
| **System-Level** | (default) | `/productspecs InventorySystem` | Generate all modules (backward compatible) |
| **Module-Level** | `--module` | `/productspecs INV --module MOD-INV-SEARCH-01` | Regenerate single module |
| **Feature-Level** | `--feature` | `/productspecs INV --feature SEARCH` | Generate all modules for a feature |
| **Screen-Level** | `--screen` | `/productspecs INV --screen SCR-003` | Generate modules for a specific screen |
| **Persona-Level** | `--persona` | `/productspecs INV --persona admin` | Generate modules used by a persona |
| **Subsystem-Level** | `--subsystem` | `/productspecs INV --subsystem middleware` | Generate modules in a subsystem |
| **Layer-Level** | `--layer` | `/productspecs INV --layer frontend` | Generate all frontend modules |

### Quality Critical Flag

```bash
/productspecs InventorySystem --quality critical
```

**Effect**: ALL modules get VP review (P0, P1, P2), per-module reviews

**Implementation**: The master orchestrator parses this flag and sets `quality_critical: true` in the filtered scope, which is then passed to all sub-orchestrators. This triggers per-module VP review for ALL modules, regardless of priority or self-validation score.

### Flag Parsing (Before CP-0)

**In productspecs.md command**:

```python
# Parse command arguments
import argparse

parser = argparse.ArgumentParser()
parser.add_argument("system_name", help="System name (e.g., InventorySystem)")
parser.add_argument("--module", help="Generate single module")
parser.add_argument("--feature", help="Generate all modules for a feature")
parser.add_argument("--screen", help="Generate modules for a screen")
parser.add_argument("--persona", help="Generate modules for a persona")
parser.add_argument("--subsystem", help="Generate modules in a subsystem")
parser.add_argument("--layer", help="Generate modules in a layer")
parser.add_argument("--quality", choices=["critical"], help="Enable VP review for ALL modules")
parser.add_argument("--from-checkpoint", type=int, help="Resume from checkpoint N")

args = parser.parse_args()

# Store flags for scope filter
flags = {
    "module": args.module,
    "feature": args.feature,
    "screen": args.screen,
    "persona": args.persona,
    "subsystem": args.subsystem,
    "layer": args.layer,
    "quality_critical": (args.quality == "critical"),
    "from_checkpoint": args.from_checkpoint
}
```

### Scope Filtering Algorithm

**Before CP-0**, the main session must run this scope filter:

```python
def filter_scope(system_name, flags):
    # Load registries
    req_registry = load_json("traceability/requirements_registry.json")
    module_registry = load_json("traceability/module_registry.json")  # From previous run or empty

    # Apply filter
    if flags.module:
        modules = [m for m in module_registry if m["id"] == flags.module]
    elif flags.feature:
        modules = [m for m in module_registry if flags.feature.lower() in m["id"].lower()]
    elif flags.screen:
        modules = [m for m in module_registry if flags.screen in m["sources"]["screens"]]
    elif flags.persona:
        modules = [m for m in module_registry if flags.persona in m["personas"]]
    elif flags.subsystem:
        modules = [m for m in module_registry if m["subsystem"] == flags.subsystem]
    elif flags.layer:
        modules = [m for m in module_registry if m["layer"] == flags.layer]
    else:
        modules = module_registry  # All modules (system-level)

    # Validate scope
    if len(modules) == 0:
        raise ValueError(f"No modules found for scope: {flags}")

    # Add VP review flags
    for module in modules:
        module["needs_vp_review"] = (
            flags.quality_critical or  # --quality critical
            module["priority"] == "P0"  # Always review P0
        )

    return {
        "type": get_filter_type(flags),  # "module", "feature", "system", etc.
        "value": get_filter_value(flags),
        "modules": modules,
        "total_modules": len(modules),
        "quality_critical": flags.quality_critical
    }
```

**Usage in Main Session**:

```python
# Parse flags from command
filtered_scope = filter_scope(system_name, flags)

# Pass to sub-orchestrators
# filtered_scope["modules"] contains only modules in scope
# filtered_scope["quality_critical"] determines VP review strategy
```

---

## ProductSpecs Pipeline Overview

### 9 Checkpoints

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PRODUCTSPECS ORCHESTRATION FLOW                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  CP-0: Initialize                                                            â”‚
â”‚    â””â”€> Create folders, state files, validate structure                      â”‚
â”‚                                                                              â”‚
â”‚  CP-1: Validate Inputs [BLOCKING]                                            â”‚
â”‚    â””â”€> Check Discovery CP-11, Prototype CP-14, load registries              â”‚
â”‚                                                                              â”‚
â”‚  CP-2: Extract Requirements                                                  â”‚
â”‚    â””â”€> Load and prioritize requirements from Discovery/Prototype            â”‚
â”‚                                                                              â”‚
â”‚  CP-3-4: Generate Modules (PARALLEL)                                         â”‚
â”‚    â”œâ”€> ui-module-specifier (UI/Screen modules)                              â”‚
â”‚    â”œâ”€> api-module-specifier (API/Backend modules)                           â”‚
â”‚    â””â”€> nfr-generator (Non-Functional Requirements)                          â”‚
â”‚    MERGE GATE â†’ module-index.md, module_registry.json                       â”‚
â”‚                                                                              â”‚
â”‚  CP-5: Generate API Contracts                                                â”‚
â”‚    â””â”€> Consolidate API specs, generate OpenAPI schemas                      â”‚
â”‚                                                                              â”‚
â”‚  CP-6: Generate Tests (PARALLEL)                                             â”‚
â”‚    â”œâ”€> unit-test-specifier (Unit test specs)                                â”‚
â”‚    â”œâ”€> integration-test-specifier (Integration test specs)                  â”‚
â”‚    â”œâ”€> e2e-test-specifier (E2E test specs)                                  â”‚
â”‚    â””â”€> pict-tester (Combinatorial test cases)                               â”‚
â”‚    MERGE GATE â†’ test-case-registry.md, test_case_registry.json              â”‚
â”‚                                                                              â”‚
â”‚  CP-7: Validate [BLOCKING - PARALLEL]                                        â”‚
â”‚    â”œâ”€> trace-validator (Traceability chain validation)                      â”‚
â”‚    â”œâ”€> cross-ref-validator (ID reference integrity)                         â”‚
â”‚    â””â”€> spec-reviewer (Spec completeness review)                             â”‚
â”‚    BLOCKING GATE â†’ 100% P0 coverage, no dangling refs                       â”‚
â”‚                                                                              â”‚
â”‚  CP-8: Export JIRA                                                           â”‚
â”‚    â””â”€> Generate CSV/JSON for JIRA import                                    â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Checkpoint Specifications

### CP-0: Initialize

**Purpose**: Create output folder structure and initialize state tracking.

**Agent Required**: No agent (main session creates files directly)

**Actions**:
```bash
# 1. Log checkpoint start
CHECKPOINT_EVENT_ID=$(python3 .claude/hooks/command_start.py \
  --command-name "/productspecs-checkpoint-0" \
  --stage "productspecs" \
  --system-name "${SystemName}" \
  --intent "Initialize ProductSpecs folder structure and state files")

# 2. Create folder structure
mkdir -p "ProductSpecs_${SystemName}/00-overview"
mkdir -p "ProductSpecs_${SystemName}/01-modules/ui"
mkdir -p "ProductSpecs_${SystemName}/01-modules/api"
mkdir -p "ProductSpecs_${SystemName}/02-api"
mkdir -p "ProductSpecs_${SystemName}/03-tests/unit"
mkdir -p "ProductSpecs_${SystemName}/03-tests/integration"
mkdir -p "ProductSpecs_${SystemName}/03-tests/e2e"
mkdir -p "ProductSpecs_${SystemName}/04-jira"
mkdir -p "_state"
mkdir -p "traceability"

# 3. Initialize state file
cat > _state/productspecs_progress.json <<EOF
{
  "system_name": "${SystemName}",
  "started_at": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
  "current_checkpoint": 0,
  "status": "in_progress",
  "checkpoints": {
    "0": {"status": "completed", "completed_at": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"}
  }
}
EOF

# 4. Create config file
cat > _state/productspecs_config.json <<EOF
{
  "system_name": "${SystemName}",
  "discovery_path": "ClientAnalysis_${SystemName}/",
  "prototype_path": "Prototype_${SystemName}/",
  "output_path": "ProductSpecs_${SystemName}/",
  "headless_mode": ${HEADLESS_MODE}
}
EOF

# 5. Log checkpoint end
python3 .claude/hooks/command_end.py \
  --command-name "/productspecs-checkpoint-0" \
  --stage "productspecs" \
  --status "completed" \
  --start-event-id "$CHECKPOINT_EVENT_ID" \
  --outputs '{"checkpoint": 0, "folders_created": 7, "state_files_created": 2}'

# 6. Update progress state
# Set current_checkpoint to 1
```

**Outputs**:
- `ProductSpecs_${SystemName}/` folder structure
- `_state/productspecs_progress.json`
- `_state/productspecs_config.json`

**Quality Gate**: All folders exist, state files valid JSON

---

### CP-1: Validate Inputs [BLOCKING]

**Purpose**: Validate that Discovery and Prototype stages are complete and load required registries.

**Agent Required**: No agent (main session validates directly)

**Actions**:
```bash
# 1. Log checkpoint start
CHECKPOINT_EVENT_ID=$(python3 .claude/hooks/command_start.py \
  --command-name "/productspecs-checkpoint-1" \
  --stage "productspecs" \
  --system-name "${SystemName}" \
  --intent "Validate Discovery and Prototype completion (BLOCKING gate)")

# 2. Check Discovery completion
if [ ! -f "ClientAnalysis_${SystemName}/05-documentation/VALIDATION_REPORT.md" ]; then
  echo "âŒ BLOCKED: Discovery CP-11 not complete"
  exit 1
fi

# 3. Check Prototype completion
if [ ! -f "Prototype_${SystemName}/14-release/BUILD_VERIFICATION.md" ]; then
  echo "âŒ BLOCKED: Prototype CP-14 not complete"
  exit 1
fi

# 4. Load required registries
DISCOVERY_SUMMARY="_state/discovery_summary.json"
SCREEN_REGISTRY="traceability/${SystemName}_screen_registry.json"
JTBD_REGISTRY="traceability/${SystemName}_jtbd_registry.json"

if [ ! -f "$DISCOVERY_SUMMARY" ]; then
  echo "âŒ BLOCKED: discovery_summary.json missing"
  exit 1
fi

# 5. Extract key metrics
SCREEN_COUNT=$(jq '.screens | length' "$SCREEN_REGISTRY")
JTBD_COUNT=$(jq '.jtbd | length' "$JTBD_REGISTRY")

echo "âœ… Discovery complete: $JTBD_COUNT JTBDs, $SCREEN_COUNT screens"

# 6. Log checkpoint end
python3 .claude/hooks/command_end.py \
  --command-name "/productspecs-checkpoint-1" \
  --stage "productspecs" \
  --status "completed" \
  --start-event-id "$CHECKPOINT_EVENT_ID" \
  --outputs "{\"checkpoint\": 1, \"screens\": ${SCREEN_COUNT}, \"jtbd\": ${JTBD_COUNT}}"

# 7. Update progress state
# Set current_checkpoint to 2
```

**Outputs**:
- Validation report (logged to console)

**Quality Gate** (BLOCKING):
- Discovery CP-11 complete
- Prototype CP-14 complete
- Required registries exist

---

### CP-2: Extract Requirements

**Purpose**: Extract and prioritize requirements from Discovery and Prototype outputs.

**Agent Required**: No agent (main session uses ProductSpecs_ExtractRequirements skill)

**Actions**:
```bash
# 1. Log checkpoint start
CHECKPOINT_EVENT_ID=$(python3 .claude/hooks/command_start.py \
  --command-name "/productspecs-checkpoint-2" \
  --stage "productspecs" \
  --system-name "${SystemName}" \
  --intent "Extract and prioritize requirements from Discovery and Prototype")

# 2. Run extraction skill
# (Main session invokes ProductSpecs_ExtractRequirements skill directly)

# 3. Generate requirements registry
# Output: traceability/requirements_registry.json

# 4. Validate outputs
python3 .claude/hooks/productspecs_quality_gates.py \
  --validate-checkpoint 2 \
  --dir "ProductSpecs_${SystemName}/"

if [ $? -ne 0 ]; then
  echo "âŒ CP-2 validation failed"
  python3 .claude/hooks/command_end.py \
    --command-name "/productspecs-checkpoint-2" \
    --stage "productspecs" \
    --status "failed" \
    --start-event-id "$CHECKPOINT_EVENT_ID" \
    --outputs '{"checkpoint": 2, "status": "validation_failed"}'
  exit 1
fi

# 5. Log checkpoint end
python3 .claude/hooks/command_end.py \
  --command-name "/productspecs-checkpoint-2" \
  --stage "productspecs" \
  --status "completed" \
  --start-event-id "$CHECKPOINT_EVENT_ID" \
  --outputs '{"checkpoint": 2, "requirements_extracted": true}'

# 6. Update progress state
# Set current_checkpoint to 3
```

**Outputs**:
- `traceability/requirements_registry.json`

**Quality Gate**:
- Requirements registry exists
- P0/P1/P2 priorities assigned
- All requirements have JTBD/screen refs

---

### CP-3-4: Generate Modules (PARALLEL)

**Purpose**: Generate UI, API, and NFR specifications in parallel **using module-orchestrator**.

**âš ï¸ NEW in v4.0.0**: This checkpoint now spawns `productspecs-module-orchestrator` sub-orchestrator, which handles:
- Spawning UI/API/NFR agents in parallel
- Self-validation (15 checks) per module
- VP review auto-trigger (score < 70 or P0)
- Batch VP review (P1/P2 at checkpoint end)
- Merge gate consolidation

**Agent Required**: `productspecs-module-orchestrator` (spawns UI/API/NFR sub-agents)

**Pre-Spawn Setup**:
```bash
# 1. Log checkpoint start
CHECKPOINT_EVENT_ID=$(python3 .claude/hooks/command_start.py \
  --command-name "/productspecs-checkpoint-3-4" \
  --stage "productspecs" \
  --system-name "${SystemName}" \
  --intent "Generate module specifications with self-validation and VP review")

# 2. Load requirements registry and filtered scope
REQUIREMENTS="traceability/requirements_registry.json"
FILTERED_SCOPE=$(cat _state/filtered_scope.json)  # From scope filter
```

**Module Orchestrator Spawn Spec**:

```javascript
Task({
  subagent_type: "general-purpose",
  model: "sonnet",
  description: "Coordinate module generation with self-validation and VP review",
  prompt: `Agent: productspecs-module-orchestrator
Read: .claude/agents/productspecs-module-orchestrator.md

## Context
System: ${SystemName}
Stage: ProductSpecs - Checkpoint 3-4 (Module Generation)

## Input
Filtered scope: ${JSON.stringify(filtered_scope.modules)}
Quality critical: ${filtered_scope.quality_critical}
Requirements: traceability/requirements_registry.json
Prototype screens: Prototype_${SystemName}/05-screens/

## VP Review Strategy
${filtered_scope.quality_critical
  ? "ALL modules get per-module VP review (P0, P1, P2)"
  : "P0 modules: per-module VP review (auto). P1/P2: batch review at checkpoint end"}

## Task
Coordinate module generation with hierarchical orchestration:
1. Spawn UI/API/NFR agents in parallel (3 agents)
2. For each module:
   - Generate module spec
   - Self-validate (15 checks via Haiku)
   - VP review triggers:
     * IF quality_critical=true â†’ VP review (all modules)
     * ELSE IF score < 70 â†’ VP review (auto-trigger)
     * ELSE IF priority=P0 â†’ VP review (mandatory)
3. Merge gate: Consolidate into module-index.md, module_registry.json
4. ${filtered_scope.quality_critical
     ? "NO batch review (all modules already reviewed per-module)"
     : "Batch VP review (P1/P2 with score â‰¥ 70) at checkpoint end"}

Return JSON: {
  "modules_generated": N,
  "vp_reviews_triggered": N,
  "p0_modules": N,
  "p1_modules": N,
  "p2_modules": N,
  "quality_critical_mode": ${filtered_scope.quality_critical},
  "files_written": [...]
}`
})
```

---

**Alternative: Direct Agent Spawning (Backward Compatible)**

If NOT using module-orchestrator, spawn UI/API/NFR agents directly:

**Agent Spawn Specs**:

#### UI Module Specifier

```javascript
Task({
  subagent_type: "general-purpose",
  model: "sonnet",
  description: "Generate UI module specifications",
  prompt: `Agent: productspecs-ui-module-specifier
Read: .claude/agents/productspecs-ui-module-specifier.md

## Context
System: ${SystemName}
Stage: ProductSpecs - Checkpoint 3-4 (Parallel Group: module-gen)

## Input
Read: Prototype_${SystemName}/05-screens/*.md
Read: traceability/requirements_registry.json
Read: traceability/${SystemName}_screen_registry.json

## Task
Generate detailed UI/Screen module specifications:
1. For each screen from Prototype, create module spec
2. Include acceptance criteria from requirements
3. Define component integration patterns
4. Specify state management approach
5. Document user flows and navigation

## Output
Write to: ProductSpecs_${SystemName}/01-modules/ui/MOD-UI-{SCREEN}-NN.md
Format: MOD-{PORTAL}-{FEATURE}-NN

Use skill: ProductSpecs_UIModuleSpecifier
Ensure: Every module â†’ REQ-XXX, SCR-XXX traceability`
})
```

#### API Module Specifier

```javascript
Task({
  subagent_type: "general-purpose",
  model: "sonnet",
  description: "Generate API module specifications",
  prompt: `Agent: productspecs-api-module-specifier
Read: .claude/agents/productspecs-api-module-specifier.md

## Context
System: ${SystemName}
Stage: ProductSpecs - Checkpoint 3-4 (Parallel Group: module-gen)

## Input
Read: Prototype_${SystemName}/04-implementation/api-contracts.md
Read: Prototype_${SystemName}/04-implementation/data-model.md
Read: traceability/requirements_registry.json

## Task
Generate detailed API/Backend module specifications:
1. For each API endpoint, create module spec
2. Include validation rules and error handling
3. Define data transformation logic
4. Specify security requirements
5. Document integration points

## Output
Write to: ProductSpecs_${SystemName}/01-modules/api/MOD-API-{ENDPOINT}-NN.md
Format: MOD-API-{RESOURCE}-{ACTION}-NN

Use skill: ProductSpecs_APIModuleSpecifier
Ensure: Every module â†’ REQ-XXX, API endpoint traceability`
})
```

#### NFR Generator

```javascript
Task({
  subagent_type: "general-purpose",
  model: "sonnet",
  description: "Generate NFR specifications",
  prompt: `Agent: productspecs-nfr-generator
Read: .claude/agents/productspecs-nfr-generator.md

## Context
System: ${SystemName}
Stage: ProductSpecs - Checkpoint 3-4 (Parallel Group: module-gen)

## Input
Read: ClientAnalysis_${SystemName}/03-strategy/KPIS_AND_GOALS.md
Read: Prototype_${SystemName}/04-implementation/data-model.md

## Task
Generate Non-Functional Requirements specifications:
1. Performance (response time, throughput)
2. Security (authentication, authorization, encryption)
3. Reliability (uptime, failover, recovery)
4. Usability (WCAG 2.1 AA compliance)
5. Maintainability (code quality, documentation)

## Output
Write to: ProductSpecs_${SystemName}/02-api/NFR_SPECIFICATIONS.md
Register: traceability/nfr_registry.json

Use skill: ProductSpecs_NFRGenerator
Format: NFR-XXX IDs with measurable acceptance criteria`
})
```

**Post-Completion Logging & Merge Gate**:
```bash
# After ALL 3 agents complete:

# 1. Log agent completions (one per agent)
python3 _state/spawn_agent_with_logging.py \
  --action complete \
  --stage "productspecs" \
  --system-name "${SystemName}" \
  --agent-type "productspecs-ui-module-specifier" \
  --task-id "${TASK_ID_1}" \
  --checkpoint 3 \
  --status "completed"

python3 _state/spawn_agent_with_logging.py \
  --action complete \
  --stage "productspecs" \
  --system-name "${SystemName}" \
  --agent-type "productspecs-api-module-specifier" \
  --task-id "${TASK_ID_2}" \
  --checkpoint 3 \
  --status "completed"

python3 _state/spawn_agent_with_logging.py \
  --action complete \
  --stage "productspecs" \
  --system-name "${SystemName}" \
  --agent-type "productspecs-nfr-generator" \
  --task-id "${TASK_ID_3}" \
  --checkpoint 3 \
  --status "completed"

# 2. MERGE GATE: Consolidate parallel agent outputs
# Collect all module specs
UI_MODULES=$(find "ProductSpecs_${SystemName}/01-modules/ui/" -name "MOD-*.md")
API_MODULES=$(find "ProductSpecs_${SystemName}/01-modules/api/" -name "MOD-*.md")

# Generate module index
cat > "ProductSpecs_${SystemName}/01-modules/module-index.md" <<EOF
# Module Index

## UI Modules
$(for MOD in $UI_MODULES; do echo "- [$(basename $MOD)]($MOD)"; done)

## API Modules
$(for MOD in $API_MODULES; do echo "- [$(basename $MOD)]($MOD)"; done)

## NFRs
- [NFR_SPECIFICATIONS.md](../02-api/NFR_SPECIFICATIONS.md)
EOF

# Update consolidated registry
# Merge outputs into traceability/module_registry.json

# 3. Validate outputs
python3 .claude/hooks/productspecs_quality_gates.py \
  --validate-checkpoint 4 \
  --dir "ProductSpecs_${SystemName}/"

if [ $? -ne 0 ]; then
  echo "âŒ CP-3-4 validation failed"
  python3 .claude/hooks/command_end.py \
    --command-name "/productspecs-checkpoint-3-4" \
    --stage "productspecs" \
    --status "failed" \
    --start-event-id "$CHECKPOINT_EVENT_ID" \
    --outputs '{"checkpoint": "3-4", "status": "validation_failed"}'
  exit 1
fi

# 4. Log checkpoint end
python3 .claude/hooks/command_end.py \
  --command-name "/productspecs-checkpoint-3-4" \
  --stage "productspecs" \
  --status "completed" \
  --start-event-id "$CHECKPOINT_EVENT_ID" \
  --outputs "{\"checkpoint\": \"3-4\", \"modules\": $(find ProductSpecs_${SystemName}/01-modules -name 'MOD-*.md' | wc -l), \"vp_reviews\": 0}"

# 5. Update progress state
# Set current_checkpoint to 5
```

**Outputs**:
- `01-modules/ui/MOD-UI-*.md` (multiple files)
- `01-modules/api/MOD-API-*.md` (multiple files)
- `02-api/NFR_SPECIFICATIONS.md`
- `01-modules/module-index.md` (merged)
- `traceability/module_registry.json` (consolidated)

**Quality Gate**:
- All modules have MOD-XXX-XXX-NN format
- All modules reference requirements (REQ-XXX)
- NFRs have measurable acceptance criteria

---

### CP-5: Generate API Contracts

**Purpose**: Consolidate API specifications and generate OpenAPI schemas.

**Agent Required**: No agent (main session consolidates)

**Actions**:
```bash
# 1. Log checkpoint start
CHECKPOINT_EVENT_ID=$(python3 .claude/hooks/command_start.py \
  --command-name "/productspecs-checkpoint-5" \
  --stage "productspecs" \
  --system-name "${SystemName}" \
  --intent "Consolidate API specifications and generate OpenAPI schemas")

# 2. Aggregate API specs from modules
# Read all MOD-API-*.md files
# Generate consolidated API index

# 3. Generate OpenAPI schemas (if applicable)

# 4. Validate outputs
python3 .claude/hooks/productspecs_quality_gates.py \
  --validate-checkpoint 5 \
  --dir "ProductSpecs_${SystemName}/"

if [ $? -ne 0 ]; then
  echo "âš ï¸ CP-5 validation warnings detected"
fi

# 5. Log checkpoint end
python3 .claude/hooks/command_end.py \
  --command-name "/productspecs-checkpoint-5" \
  --stage "productspecs" \
  --status "completed" \
  --start-event-id "$CHECKPOINT_EVENT_ID" \
  --outputs '{"checkpoint": 5, "api_index_created": true}'

# 6. Update progress state
# Set current_checkpoint to 6
```

**Outputs**:
- `02-api/api-index.md`

**Quality Gate**: API index exists

---

### CP-6: Generate Tests (PARALLEL)

**Purpose**: Generate unit, integration, E2E, and combinatorial test specifications in parallel **using test-orchestrator**.

**âš ï¸ NEW in v4.0.0**: This checkpoint now spawns `productspecs-test-orchestrator` sub-orchestrator, which handles:
- Spawning Unit/Integration/E2E/PICT agents in parallel (4 agents)
- Self-validation per test spec
- Test coverage analysis (P0 modules require unit + E2E)
- Merge gate consolidation

**Agent Required**: `productspecs-test-orchestrator` (spawns Unit/Integration/E2E/PICT sub-agents)

**Pre-Spawn Setup**:
```bash
# 1. Log checkpoint start
CHECKPOINT_EVENT_ID=$(python3 .claude/hooks/command_start.py \
  --command-name "/productspecs-checkpoint-6" \
  --stage "productspecs" \
  --system-name "${SystemName}" \
  --intent "Generate test specifications (Unit, Integration, E2E, PICT)")

# 2. Load module registry and filtered scope
MODULES="traceability/module_registry.json"
FILTERED_SCOPE=$(cat _state/filtered_scope.json)  # From scope filter
```

**Test Orchestrator Spawn Spec**:

```javascript
Task({
  subagent_type: "general-purpose",
  model: "sonnet",
  description: "Coordinate test generation with self-validation",
  prompt: `Agent: productspecs-test-orchestrator
Read: .claude/agents/productspecs-test-orchestrator.md

## Context
System: ${SystemName}
Stage: ProductSpecs - Checkpoint 6 (Test Generation)

## Input
Module registry: traceability/module_registry.json
Filtered scope: ${filtered_scope ? JSON.stringify(filtered_scope.modules) : "all modules"}

## Task
Coordinate test generation with hierarchical orchestration:
1. Spawn Unit/Integration/E2E/PICT agents in parallel (4 agents)
2. For each test spec:
   - Generate test spec
   - Self-validate (15 checks via Haiku)
   - If validation fails â†’ retry (max 2 retries)
3. Test coverage analysis (P0 modules require unit + E2E)
4. Merge gate: Consolidate into test-case-registry.md, test_case_registry.json
5. Optional VP review for coverage gaps

Return JSON: {
  "tests_generated": N,
  "unit_tests": N,
  "integration_tests": N,
  "e2e_tests": N,
  "pict_tests": N,
  "coverage_gaps": N,
  "files_written": [...]
}`
})
```

---

**Alternative: Direct Agent Spawning (Backward Compatible)**

If NOT using test-orchestrator, spawn Unit/Integration/E2E/PICT agents directly:

**Agent Spawn Specs**:

#### Unit Test Specifier

```javascript
Task({
  subagent_type: "general-purpose",
  model: "sonnet",
  description: "Generate unit test specifications",
  prompt: `Agent: productspecs-unit-test-specifier
Read: .claude/agents/productspecs-unit-test-specifier.md

## Context
System: ${SystemName}
Stage: ProductSpecs - Checkpoint 6 (Parallel Group: test-gen)

## Input
Read: ProductSpecs_${SystemName}/01-modules/**/*.md
Read: traceability/module_registry.json

## Task
Generate unit test specifications:
1. For each module, create unit test spec
2. Cover all functions, components, classes
3. Include edge cases, error paths, boundary conditions
4. Define test data requirements
5. Specify mocking strategy

## Output
Write to: ProductSpecs_${SystemName}/03-tests/unit/UT-{MODULE-ID}.md
Format: UT-XXX for each test case

Use skill: ProductSpecs_UnitTestSpecifier
Ensure: Every test â†’ MOD-XXX-XXX-NN traceability`
})
```

#### Integration Test Specifier

```javascript
Task({
  subagent_type: "general-purpose",
  model: "sonnet",
  description: "Generate integration test specs",
  prompt: `Agent: productspecs-integration-test-specifier
Read: .claude/agents/productspecs-integration-test-specifier.md

## Context
System: ${SystemName}
Stage: ProductSpecs - Checkpoint 6 (Parallel Group: test-gen)

## Input
Read: ProductSpecs_${SystemName}/02-api/*.md
Read: Prototype_${SystemName}/04-implementation/api-contracts.md

## Task
Generate integration test specifications:
1. Test API flows end-to-end
2. Validate data transformations
3. Test service interactions
4. Verify cross-module communication

## Output
Write to: ProductSpecs_${SystemName}/03-tests/integration/IT-{FLOW}.md
Format: IT-XXX for each integration test

Use skill: ProductSpecs_IntegrationTestSpecifier
Ensure: Every test â†’ MOD-XXX or API endpoint traceability`
})
```

#### E2E Test Specifier

```javascript
Task({
  subagent_type: "general-purpose",
  model: "sonnet",
  description: "Generate E2E test specifications",
  prompt: `Agent: productspecs-e2e-test-specifier
Read: .claude/agents/productspecs-e2e-test-specifier.md

## Context
System: ${SystemName}
Stage: ProductSpecs - Checkpoint 6 (Parallel Group: test-gen)

## Input
Read: ClientAnalysis_${SystemName}/02-research/JOBS_TO_BE_DONE.md
Read: Prototype_${SystemName}/05-screens/*.md

## Task
Generate E2E test specifications:
1. Map JTBDs to user journeys
2. Define complete workflows (login â†’ task â†’ logout)
3. Specify validation points at each step
4. Include happy path and error scenarios

## Output
Write to: ProductSpecs_${SystemName}/03-tests/e2e/E2E-{JOURNEY}.md
Format: E2E-XXX for each journey

Use skill: ProductSpecs_E2ETestSpecifier
Ensure: Every test â†’ JTBD-X.Y traceability`
})
```

#### PICT Tester

```javascript
Task({
  subagent_type: "general-purpose",
  model: "haiku",  // Combinatorial calculation task
  description: "Generate PICT combinatorial tests",
  prompt: `Agent: productspecs-pict-tester
Read: .claude/agents/productspecs-pict-tester.md

## Context
System: ${SystemName}
Stage: ProductSpecs - Checkpoint 6 (Parallel Group: test-gen)

## Input
Read: ProductSpecs_${SystemName}/01-modules/**/*.md

## Task
Generate combinatorial test cases using PICT:
1. Identify parameters with multiple values
2. Create PICT model files
3. Generate pairwise test combinations
4. Map combinations to test cases

## Output
Write to: ProductSpecs_${SystemName}/03-tests/pict/PICT_COMBINATIONS.md
Include: PICT model files and generated combinations

Use skill: ProductSpecs_PICTTester
Format: PICT-XXX for combinatorial test IDs`
})
```

**Post-Completion Logging & Merge Gate**:
```bash
# After ALL 4 agents complete:

# 1. Log agent completions (one per agent)
for AGENT in "unit-test-specifier" "integration-test-specifier" "e2e-test-specifier" "pict-tester"; do
  python3 _state/spawn_agent_with_logging.py \
    --action complete \
    --stage "productspecs" \
    --system-name "${SystemName}" \
    --agent-type "productspecs-${AGENT}" \
    --task-id "${TASK_ID}" \
    --checkpoint 6 \
    --status "completed"
done

# 2. MERGE GATE: Consolidate test outputs
# Collect all test specs
UNIT_TESTS=$(find "ProductSpecs_${SystemName}/03-tests/unit/" -name "UT-*.md")
INT_TESTS=$(find "ProductSpecs_${SystemName}/03-tests/integration/" -name "IT-*.md")
E2E_TESTS=$(find "ProductSpecs_${SystemName}/03-tests/e2e/" -name "E2E-*.md")

# Generate test case registry
cat > "ProductSpecs_${SystemName}/03-tests/test-case-registry.md" <<EOF
# Test Case Registry

## Unit Tests ($(echo $UNIT_TESTS | wc -w))
$(for TEST in $UNIT_TESTS; do echo "- [$(basename $TEST)]($TEST)"; done)

## Integration Tests ($(echo $INT_TESTS | wc -w))
$(for TEST in $INT_TESTS; do echo "- [$(basename $TEST)]($TEST)"; done)

## E2E Tests ($(echo $E2E_TESTS | wc -w))
$(for TEST in $E2E_TESTS; do echo "- [$(basename $TEST)]($TEST)"; done)

## PICT Combinatorial Tests
- [PICT_COMBINATIONS.md](pict/PICT_COMBINATIONS.md)
EOF

# Update test case registry JSON
# Merge into traceability/test_case_registry.json

# 3. Validate outputs
python3 .claude/hooks/productspecs_quality_gates.py \
  --validate-checkpoint 6 \
  --dir "ProductSpecs_${SystemName}/"

if [ $? -ne 0 ]; then
  echo "âŒ CP-6 validation failed"
  python3 .claude/hooks/command_end.py \
    --command-name "/productspecs-checkpoint-6" \
    --stage "productspecs" \
    --status "failed" \
    --start-event-id "$CHECKPOINT_EVENT_ID" \
    --outputs '{"checkpoint": 6, "status": "validation_failed"}'
  exit 1
fi

# 4. Log checkpoint end
python3 .claude/hooks/command_end.py \
  --command-name "/productspecs-checkpoint-6" \
  --stage "productspecs" \
  --status "completed" \
  --start-event-id "$CHECKPOINT_EVENT_ID" \
  --outputs "{\"checkpoint\": 6, \"tests\": $(find ProductSpecs_${SystemName}/03-tests -name 'TC-*.md' | wc -l)}"

# 5. Update progress state
# Set current_checkpoint to 7
```

**Outputs**:
- `03-tests/unit/UT-*.md` (multiple files)
- `03-tests/integration/IT-*.md` (multiple files)
- `03-tests/e2e/E2E-*.md` (multiple files)
- `03-tests/pict/PICT_COMBINATIONS.md`
- `03-tests/test-case-registry.md` (merged)
- `traceability/test_case_registry.json` (consolidated)

**Quality Gate**:
- All tests have UT/IT/E2E-XXX format
- All tests reference modules or JTBDs
- PICT combinations generated

---

### CP-7: Validate [BLOCKING - PARALLEL]

**Purpose**: Validate traceability, cross-references, and specification completeness in parallel **using validation-orchestrator**.

**âš ï¸ NEW in v4.0.0**: This checkpoint now spawns `productspecs-validation-orchestrator` sub-orchestrator, which handles:
- Spawning 3 validators in parallel (traceability, cross-reference, spec-reviewer)
- Blocking gate checks (P0 coverage = 100%, dangling refs = 0, quality â‰¥ 70)
- BLOCKS progression if criteria not met

**Agent Required**: `productspecs-validation-orchestrator` (spawns 3 validator sub-agents)

**Pre-Spawn Setup**:
```bash
# Log checkpoint start
CHECKPOINT_EVENT_ID=$(python3 .claude/hooks/command_start.py \
  --command-name "/productspecs-checkpoint-7" \
  --stage "productspecs" \
  --system-name "${SystemName}" \
  --intent "BLOCKING validation gate (P0 coverage, dangling refs, quality score)")

echo "Starting ProductSpecs Checkpoint 7: BLOCKING validation gate"
```

**Validation Orchestrator Spawn Spec**:

```javascript
Task({
  subagent_type: "general-purpose",
  model: "sonnet",
  description: "Coordinate validation with blocking gate",
  prompt: `Agent: productspecs-validation-orchestrator
Read: .claude/agents/productspecs-validation-orchestrator.md

## Context
System: ${SystemName}
Stage: ProductSpecs - Checkpoint 7 (BLOCKING Validation Gate)

## Input
Module registry: traceability/module_registry.json
Test registry: traceability/test_case_registry.json
Output path: ProductSpecs_${SystemName}/

## Task
Coordinate validation with blocking gate:
1. Spawn 3 validators in parallel:
   - traceability-validator (PPâ†’JTBDâ†’REQâ†’MODâ†’TC chains)
   - cross-reference-validator (ID integrity)
   - spec-reviewer (quality review)
2. Analyze validation results
3. Check blocking criteria:
   - P0 coverage = 100% (CRITICAL)
   - Dangling references = 0 (CRITICAL)
   - Quality score â‰¥ 70 (CRITICAL)
4. Generate blocking gate report
5. BLOCK if any criteria fail

Return JSON: {
  "status": "PASS" | "BLOCKED",
  "blocking_issues": [...],
  "validation_results": {
    "p0_coverage": N,
    "dangling_refs": N,
    "avg_quality_score": N
  },
  "files_written": [...]
}

**CRITICAL**: If status = "BLOCKED", DO NOT proceed to CP-8.`
})
```

---

**Alternative: Direct Agent Spawning (Backward Compatible)**

If NOT using validation-orchestrator, spawn 3 validators directly:

**Agent Spawn Specs**:

#### Trace Validator

```javascript
Task({
  subagent_type: "general-purpose",
  model: "haiku",  // Checklist-based validation
  description: "Validate traceability chains",
  prompt: `Agent: productspecs-trace-validator
Read: .claude/agents/productspecs-trace-validator.md

## Context
System: ${SystemName}
Stage: ProductSpecs - Checkpoint 7 (Parallel Group: validation, BLOCKING)

## Input
Read ALL files in: ProductSpecs_${SystemName}/

## Task
Validate complete traceability chains:
1. PP-X.Y â†’ JTBD-X.Y â†’ REQ-XXX â†’ MOD-XXX â†’ TC-XXX
2. Every P0 requirement has module(s)
3. Every P0 module has test case(s)
4. No orphaned requirements or modules

## Output
Write to: ProductSpecs_${SystemName}/00-overview/TRACEABILITY_REPORT.md
Report: Coverage %, orphans, broken chains

Use skill: ProductSpecs_TraceValidator
**BLOCKING**: Must show 100% P0 coverage`
})
```

#### Cross-Reference Validator

```javascript
Task({
  subagent_type: "general-purpose",
  model: "haiku",  // Checklist-based validation
  description: "Validate ID references",
  prompt: `Agent: productspecs-cross-ref-validator
Read: .claude/agents/productspecs-cross-ref-validator.md

## Context
System: ${SystemName}
Stage: ProductSpecs - Checkpoint 7 (Parallel Group: validation, BLOCKING)

## Input
Read ALL files in: ProductSpecs_${SystemName}/

## Task
Validate ID reference integrity:
1. All referenced IDs exist (REQ-XXX, MOD-XXX, TC-XXX)
2. No dangling references (ID mentioned but not defined)
3. Proper ID format (MOD-{PORTAL}-{FEATURE}-NN)
4. No circular dependencies

## Output
Write to: ProductSpecs_${SystemName}/00-overview/CROSS_REF_REPORT.md
Report: Total refs, violations, severity

Use skill: ProductSpecs_CrossRefValidator
**BLOCKING**: Must have 0 dangling references`
})
```

#### Spec Reviewer

```javascript
Task({
  subagent_type: "general-purpose",
  model: "sonnet",  // Qualitative review
  description: "Review spec completeness",
  prompt: `Agent: productspecs-spec-reviewer
Read: .claude/agents/productspecs-spec-reviewer.md

## Context
System: ${SystemName}
Stage: ProductSpecs - Checkpoint 7 (Parallel Group: validation, BLOCKING)

## Input
Read ALL module specs: ProductSpecs_${SystemName}/01-modules/**/*.md

## Task
Review specification completeness and quality:
1. All required sections present (Purpose, AC, Implementation)
2. Acceptance criteria are clear and testable
3. No placeholder text or TODOs
4. Consistent terminology usage
5. Quality score >= 70

## Output
Write to: ProductSpecs_${SystemName}/00-overview/SPEC_QUALITY_REPORT.md
Report: Quality scores, missing sections, recommendations

Use skill: ProductSpecs_SpecReviewer
**BLOCKING**: Quality score must be >= 70`
})
```

**Post-Completion Logging & Blocking Gate**:
```bash
# After ALL 3 validators complete:

# 1. Log validator completions
for AGENT in "trace-validator" "cross-ref-validator" "spec-reviewer"; do
  python3 _state/spawn_agent_with_logging.py \
    --action complete \
    --stage "productspecs" \
    --system-name "${SystemName}" \
    --agent-type "productspecs-${AGENT}" \
    --task-id "${TASK_ID}" \
    --checkpoint 7 \
    --status "completed"
done

# 2. BLOCKING GATE: Check validation results
P0_COVERAGE=$(grep "P0 Coverage" "ProductSpecs_${SystemName}/00-overview/TRACEABILITY_REPORT.md" | awk '{print $3}' | tr -d '%')
DANGLING_REFS=$(grep "Dangling References" "ProductSpecs_${SystemName}/00-overview/CROSS_REF_REPORT.md" | awk '{print $3}')
QUALITY_SCORE=$(grep "Overall Quality Score" "ProductSpecs_${SystemName}/00-overview/SPEC_QUALITY_REPORT.md" | awk '{print $4}')

if [ "$P0_COVERAGE" -lt 100 ]; then
  echo "âŒ BLOCKED: P0 coverage = ${P0_COVERAGE}% (must be 100%)"
  exit 1
fi

if [ "$DANGLING_REFS" -gt 0 ]; then
  echo "âŒ BLOCKED: ${DANGLING_REFS} dangling references (must be 0)"
  exit 1
fi

if [ "$QUALITY_SCORE" -lt 70 ]; then
  echo "âŒ BLOCKED: Quality score = ${QUALITY_SCORE} (must be >= 70)"
  exit 1
fi

echo "âœ… All validation gates passed"

# 3. Generate consolidated traceability matrix
cat > "ProductSpecs_${SystemName}/00-overview/TRACEABILITY_MATRIX.md" <<EOF
# Traceability Matrix

**P0 Coverage**: ${P0_COVERAGE}%
**Dangling References**: ${DANGLING_REFS}
**Quality Score**: ${QUALITY_SCORE}

[Include full traceability matrix here]
EOF

# 4. Validate checkpoint (BLOCKING)
python3 .claude/hooks/productspecs_quality_gates.py \
  --validate-checkpoint 7 \
  --dir "ProductSpecs_${SystemName}/"

# If validation fails, STOP - user must fix violations
if [ $? -ne 0 ]; then
  echo "âŒ BLOCKED: CP-7 validation failed"
  python3 .claude/hooks/command_end.py \
    --command-name "/productspecs-checkpoint-7" \
    --stage "productspecs" \
    --status "blocked" \
    --start-event-id "$CHECKPOINT_EVENT_ID" \
    --outputs "{\"checkpoint\": 7, \"status\": \"BLOCKED\", \"p0_coverage\": ${P0_COVERAGE}, \"dangling_refs\": ${DANGLING_REFS}, \"quality_score\": ${QUALITY_SCORE}}"
  exit 1
fi

# 5. Log checkpoint end
python3 .claude/hooks/command_end.py \
  --command-name "/productspecs-checkpoint-7" \
  --stage "productspecs" \
  --status "completed" \
  --start-event-id "$CHECKPOINT_EVENT_ID" \
  --outputs "{\"checkpoint\": 7, \"status\": \"PASS\", \"p0_coverage\": 100, \"dangling_refs\": 0, \"quality_score\": ${QUALITY_SCORE}}"

# 6. Update progress state
# Set current_checkpoint to 8
```

**Outputs**:
- `00-overview/TRACEABILITY_REPORT.md`
- `00-overview/CROSS_REF_REPORT.md`
- `00-overview/SPEC_QUALITY_REPORT.md`
- `00-overview/TRACEABILITY_MATRIX.md` (consolidated)

**Quality Gate** (BLOCKING):
- P0 coverage = 100%
- Dangling references = 0
- Quality score >= 70

---

### CP-8: Export JIRA

**Purpose**: Generate CSV and JSON files for JIRA import.

**Agent Required**: No agent (main session uses ProductSpecs_JIRAExporter skill)

**Actions**:
```bash
# 1. Log checkpoint start
CHECKPOINT_EVENT_ID=$(python3 .claude/hooks/command_start.py \
  --command-name "/productspecs-checkpoint-8" \
  --stage "productspecs" \
  --system-name "${SystemName}" \
  --intent "Generate JIRA CSV/JSON for import")

# 2. Run JIRA exporter skill
# (Main session invokes ProductSpecs_JIRAExporter skill directly)

# 3. Generate JIRA files
# - full-hierarchy.csv (Epic â†’ Story â†’ Sub-task)
# - jira-import.json
# - IMPORT_GUIDE.md

# 4. Generate summary
cat > "ProductSpecs_${SystemName}/00-overview/GENERATION_SUMMARY.md" <<EOF
# ProductSpecs Generation Summary

**System**: ${SystemName}
**Completed**: $(date)

## Outputs Generated
- Modules: $(find ProductSpecs_${SystemName}/01-modules/ -name "MOD-*.md" | wc -l)
- Test Cases: $(find ProductSpecs_${SystemName}/03-tests/ -name "*.md" | wc -l)
- JIRA Items: Ready for import

## Quality Metrics
- P0 Coverage: 100%
- Quality Score: ${QUALITY_SCORE}

## Next Steps
1. Import JIRA files: ProductSpecs_${SystemName}/04-jira/
2. Review traceability matrix: 00-overview/TRACEABILITY_MATRIX.md
3. Proceed to Solution Architecture stage
EOF

# 5. Validate outputs
python3 .claude/hooks/productspecs_quality_gates.py \
  --validate-checkpoint 8 \
  --dir "ProductSpecs_${SystemName}/"

if [ $? -ne 0 ]; then
  echo "âš ï¸ CP-8 validation warnings detected"
fi

# 6. Log checkpoint end
python3 .claude/hooks/command_end.py \
  --command-name "/productspecs-checkpoint-8" \
  --stage "productspecs" \
  --status "completed" \
  --start-event-id "$CHECKPOINT_EVENT_ID" \
  --outputs "{\"checkpoint\": 8, \"jira_files_created\": 3, \"modules\": $(find ProductSpecs_${SystemName}/01-modules -name 'MOD-*.md' | wc -l), \"tests\": $(find ProductSpecs_${SystemName}/03-tests -name 'TC-*.md' | wc -l)}"

# 7. Update progress state
# Set current_checkpoint to 9 (COMPLETE)
# Set status to "completed"
```

**Outputs**:
- `04-jira/full-hierarchy.csv`
- `04-jira/jira-import.json`
- `04-jira/IMPORT_GUIDE.md`
- `00-overview/GENERATION_SUMMARY.md`

**Quality Gate**: JIRA files valid and importable

---

## State Management

### Progress State Schema

```json
{
  "system_name": "EmergencyTriage",
  "started_at": "2026-01-10T10:00:00Z",
  "current_checkpoint": 6,
  "status": "in_progress",
  "checkpoints": {
    "0": {"status": "completed", "completed_at": "2026-01-10T10:05:00Z"},
    "1": {"status": "completed", "completed_at": "2026-01-10T10:10:00Z"},
    "2": {"status": "completed", "completed_at": "2026-01-10T10:20:00Z"},
    "3": {
      "status": "completed",
      "parallel_agents": ["ui-module-specifier", "api-module-specifier", "nfr-generator"],
      "completed_at": "2026-01-10T10:45:00Z"
    },
    "5": {"status": "completed", "completed_at": "2026-01-10T10:50:00Z"},
    "6": {
      "status": "in_progress",
      "parallel_agents": ["unit-test-specifier", "integration-test-specifier", "e2e-test-specifier", "pict-tester"],
      "agents_completed": ["unit-test-specifier", "pict-tester"],
      "agents_pending": ["integration-test-specifier", "e2e-test-specifier"]
    }
  },
  "metrics": {
    "total_modules": 23,
    "total_tests": 0,
    "p0_coverage": 0
  }
}
```

### Resume Protocol

When resuming ProductSpecs:

1. **Load** `_state/productspecs_progress.json`
2. **Find** last incomplete checkpoint (`status != "completed"`)
3. **For that checkpoint**:
   - If parallel execution, check `agents_completed` vs `parallel_agents`
   - Skip completed agents
   - Dispatch only remaining agents
4. **Continue** normal flow from that checkpoint forward

---

## Parallel Execution Strategy

### Group 1: Module Generation (CP-3-4)

```
SPAWN IN PARALLEL:
â”œâ”€â”€ ui-module-specifier (UI/Screen modules)
â”œâ”€â”€ api-module-specifier (API/Backend modules)
â””â”€â”€ nfr-generator (Non-Functional Requirements)

AWAIT ALL completions
MERGE GATE â†’ consolidate into module-index.md, module_registry.json
```

### Group 2: Test Generation (CP-6)

```
SPAWN IN PARALLEL:
â”œâ”€â”€ unit-test-specifier (unit test specs)
â”œâ”€â”€ integration-test-specifier (integration test specs)
â”œâ”€â”€ e2e-test-specifier (E2E test specs)
â””â”€â”€ pict-tester (combinatorial test cases)

AWAIT ALL completions
MERGE GATE â†’ consolidate into test-case-registry.md, test_case_registry.json
```

### Group 3: Validation (CP-7)

```
SPAWN IN PARALLEL:
â”œâ”€â”€ trace-validator (traceability chains)
â”œâ”€â”€ cross-ref-validator (ID reference integrity)
â””â”€â”€ spec-reviewer (specification quality)

AWAIT ALL completions
BLOCKING GATE â†’ check P0=100%, dangling=0, quality>=70
```

### Sequential Dependencies

```
CP-0 â†’ CP-1 â†’ CP-2 â†’ CP-3-4 â†’ CP-5 â†’ CP-6 â†’ CP-7 â†’ CP-8
```

**No parallelism** between checkpoints - each checkpoint must complete fully before next begins.

---

## Quality Gates

### Blocking Gates

| Checkpoint | Requirement |
|------------|-------------|
| CP-1 | Discovery CP-11 complete, Prototype CP-14 complete |
| CP-7 | P0 coverage = 100%, dangling refs = 0, quality >= 70 |

### Non-Blocking Validations

All other checkpoints have non-blocking validations that warn but allow progression.

---

## Traceability Chain

```
PP-X.Y (Pain Point) â†’ Discovery
    â†“
JTBD-X.Y (Job To Be Done) â†’ Discovery
    â†“
REQ-XXX (Requirement) â†’ ProductSpecs CP-2
    â†“
MOD-XXX-XXX-NN (Module) â†’ ProductSpecs CP-3-4
    â†“
TC-XXX (Test Case) â†’ ProductSpecs CP-6
```

All IDs must maintain bidirectional links.

---

## Output Structure

```
ProductSpecs_${SystemName}/
â”œâ”€â”€ 00-overview/
â”‚   â”œâ”€â”€ GENERATION_SUMMARY.md
â”‚   â”œâ”€â”€ TRACEABILITY_MATRIX.md
â”‚   â”œâ”€â”€ TRACEABILITY_REPORT.md
â”‚   â”œâ”€â”€ CROSS_REF_REPORT.md
â”‚   â””â”€â”€ SPEC_QUALITY_REPORT.md
â”œâ”€â”€ 01-modules/
â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â””â”€â”€ MOD-UI-*.md
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ MOD-API-*.md
â”‚   â””â”€â”€ module-index.md
â”œâ”€â”€ 02-api/
â”‚   â”œâ”€â”€ api-index.md
â”‚   â””â”€â”€ NFR_SPECIFICATIONS.md
â”œâ”€â”€ 03-tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â””â”€â”€ UT-*.md
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â””â”€â”€ IT-*.md
â”‚   â”œâ”€â”€ e2e/
â”‚   â”‚   â””â”€â”€ E2E-*.md
â”‚   â”œâ”€â”€ pict/
â”‚   â”‚   â””â”€â”€ PICT_COMBINATIONS.md
â”‚   â””â”€â”€ test-case-registry.md
â””â”€â”€ 04-jira/
    â”œâ”€â”€ full-hierarchy.csv
    â”œâ”€â”€ jira-import.json
    â””â”€â”€ IMPORT_GUIDE.md
```

---

## Error Handling

### Agent Failure

```
ON agent failure during parallel execution:
  1. Mark agent as failed in progress
  2. Continue waiting for other agents
  3. At merge gate:
     - If failed agent was critical: RETRY once
     - If retry fails: BLOCK with error
     - If non-critical: WARN and continue
```

### Blocking Gate Failure

```
ON CP-7 validation failure:
  1. Generate gap report with specific violations
  2. STOP execution - do not proceed to CP-8
  3. User must fix violations and re-run validation
  4. Resume from CP-7 after fixes
```

---

## Command Integration

| Command | Checkpoints | Mode |
|---------|-------------|------|
| `/productspecs` | 0-8 | Full |
| `/productspecs-resume` | Continue | Resume |
| `/productspecs-modules` | 3-4 | Modules only |
| `/productspecs-tests` | 6 | Tests only |
| `/productspecs-validate` | 7 | Validation only |
| `/productspecs-jira` | 8 | JIRA export only |

---

## Related

- **Agent Registry**: `.claude/agents/productspecs/PRODUCTSPECS_AGENT_REGISTRY.json`
- **Skills**: `.claude/skills/ProductSpecs_*/`
- **Command Reference**: `.claude/commands/PRODUCTSPECS_COMMAND_REFERENCE.md`
- **Quality Gates**: `.claude/hooks/productspecs_quality_gates.py`
- **Logging Wrapper**: `_state/spawn_agent_with_logging.py`
- **Root Cause Analysis**: `architecture/Multi_Agent_Root_Cause_Analysis.md`

---

## Available Skills

As an orchestrator, you can utilize these skills for enhanced documentation and visualization:

### Process Visualization

**When to use**: Generating workflow diagrams or process flow documentation

```bash
/flowchart-creator
```

Use to create HTML flowcharts showing ProductSpecs checkpoint progression, module generation flows, or test specification workflows.

### Progress Tracking

**When to use**: Creating visual progress dashboards for ProductSpecs phases

```bash
/dashboard-creator
```

Use to create HTML dashboards showing checkpoint completion status, P0 test coverage, or traceability metrics.

### Technical Documentation

**When to use**: Generating comprehensive technical documentation

```bash
/technical-doc-creator
```

Use to create HTML technical documentation for API module specs, data contracts, or module interfaces.

### Skill Discovery (MANDATORY)

**When to use**: Ensuring spawned agents use appropriate skills

```bash
/using-htec-accelerators
```

Reference this skill when guiding spawned agents to check for and use relevant specialized skills.

See `.claude/skills/{skill-name}/SKILL.md` for detailed usage instructions for each skill.

---

## COMPLETION LOGGING (MANDATORY)

BEFORE returning your result, run this command:

```bash
bash .claude/hooks/log-lifecycle.sh subagent productspecs-orchestrator completed '{"stage": "productspecs", "status": "completed", "files_written": ["*.md"]}'
```

Replace the files_written array with actual files you created.

---
## Execution Logging

This agent uses **deterministic lifecycle logging**.

**Events logged:**
- `subagent:productspecs-orchestrator:started` - When agent begins (via FIRST ACTION)
- `subagent:productspecs-orchestrator:completed` - When agent completes (via COMPLETION LOGGING)
- `subagent:productspecs-orchestrator:stopped` - When agent finishes (via global SubagentStop hook)
- `agent:task-spawn:pre_spawn` / `post_spawn` - When Task() tool invokes this agent (via settings.json)

**Log file:** `_state/lifecycle.json`
